# Load Balancer Service

A Load Balancer service in the kubernetes helps exposes your application to the internet, means allowing people outside your kubernetes cluster to access it. Think you have given your app a public address so anyone can reach it.

 ```Load Balancer service, kubernetes cluster ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ created application ‡§ï‡•ã internet ‡§™‡§∞ expose ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§è‡§ï public Ip ‡§ï‡•á through ‡§≤‡•ã‡§ó application ‡§ï‡•ã access ‡§ï‡§∞ ‡§∏‡§ï‡•á‡§Ç|```

 Jab aapko chahiye ki aapka application internet se directly access ho, tab aapko LoadBalancer type use karna padta hai.

<br>

### Pehle Problem Samjho ‚Äî Why Need a LoadBalancer service?

Kubernetes cluster ke andar jab hum koi web app ya API run karte hain, toh wo ek Pod ke form mein chalti hai.

Lekin:
- Pod ki IP Cluster ke bahar se access nahi hoti. Pod ki IP private hoti hai, aur sirf cluster ke andar se access ki ja sakti hai.
- Humara user to internet se aayega (ya cloud network se). Usko to access chahiye publicly ‚Äî like ```https://mywebsite.com```.
- Internal ClusterIP aur NodePort se to external client access nahi kar sakta easily.

**Solution ‚Äî Kubernetes LoadBalancer Service**

LoadBalancer service ek aisi service hoti hai jo cloud provider se ek public IP allocate karwata hai (jaise AWS, Azure, GCP), aur external clients ko allow karta hai ki wo directly Kubernetes application ko access kar sakein.

LoadBalancer service simply cloud main jake ek load balancer create kar deti hai jisse ek public ip mil jati hai aur user us public ip ka use karke application ko browser main access kar sakta hai.

<br>

### Important Point

LoadBalancer tabhi kaam karta hai jab tumhara cluster cloud environment me ho ‚Äî jaise:
- EKS (AWS).
- AKS (Azure).
- GKE (Google Cloud).

Agar tum local Minikube ya kubeadm cluster pe ho, to LoadBalancer kaam nahi karega bina MetalLB jaise tool ke.

<br>

### How does a Load Balancer created in cloud.

**Step - 1: Tumne LoadBalancer Service YAML create karke YAML Apply Kiya**:

Example:
```
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: LoadBalancer
  selector:
    app: myapp
  ports:
    - port: 80
      targetPort: 8080
```
- Jab tum YAML apply karte ho to kubectl tumhara manifest API Server ko bhejta hai.
- API Server validate karta hai:
  - Syntax theek hai?
  - Required fields present hain?
- Phir service object ko etcd database me store karta hai.

Important Point:
```
spec:
  type: LoadBalancer
```
- ye field API server dekhta hai aur ```type``` ko ```LoadBalancer``` mark karta hai, jo baad me trigger karega Cloud integration ko.


**Step - 2: Kubernetes Cloud Controller Manager (CCM) Trigger Hota Hai**:

- CCM (Cloud Controller Manager) ek Kubernetes internal component hota hai jo control plane main hota hai.
- Ye har major cloud provider (AWS, Azure, GCP) ke liye alag hota hai.
- Sirf tab active hota hai jab tum Kubernetes cluster ko kisi cloud provider par chala rahe ho (AWS, Azure, GCP, etc.).
- Ye api server ko watch loop main dekhta hai ki kya cluster me koi aisa service create hua jiska type ```LoadBalancer``` hai.
- Jaise hi aisa service dikhta hai, ye cloud ke REST API ya SDK se baat karta hai:
  - Public Load Balancer banana.
  - Nodes ke IP address aur NodePort use karke listener rules set karta hai.
  - DNS name ya public IP assign karta hai.
 
Output:
- Ek cloud-level Load Balancer ready ho jaata hai.
- Uska public IP/DNS naam mil jaata hai.

<br>
<br>

### How does Load Balancer sends traffic to pod.

Ab dekhte hain ki jab user load balancer ki public ip se application ko access karta hai to traffic pod tak kaise pahunchta hai.

**Step-1: User Sends Request to Public IP**:
- Browser se user ne ```http://<LoadBalancer-Public-IP>:80``` pe request bheji.
- Ye request pehle Cloud Load Balancer tak jaati hai.

**Step-2: Cloud Load Balancer Forwards to Kubernetes NodePort**:
- Load Balancer randomly ya round-robin basis pe ek node select karta hai (from the available pool of worker nodes).
- Us selected Node ke ```NodeIP:NodePort``` (e.g. 192.168.1.12:31234) pe request bhejta hai.

**Step-3: NodePort forwards the request to cluster ip using Kube Proxy**:
- Har worker node par ek ```kube-proxy``` hota hai. Ye kube-proxy worker node ke kernal pe ```iptables/ipvs``` rules bana deta hai.
- Aur un rules ke andar likha hota hai:
  - ```"Agar traffic Node ke port 31234 pe aaya, toh use ClusterIP (10.96.0.10) ke port 80 pe forward karo aur cluster ip ki selector list se pod select karo."```.
 
Matlab ```kube-proxy``` ke paas cluster-ip ki list hoti hai aur cluster-ip ke selector ki detail bhi hoti hai jisse kube-proxy pod ko select karta hai.

Matlab yha ```kube-proxy``` cluster-ip ke selector ki help se ye decide karta hai ki konse pod par request bheju.

Jaise hum service yaml main selector ka use karke btate hain ki konse pod se ye service attach hogi, to ye hi list kube-proxy ke paas hoti hai.

Isi list ko use karke use karke ```kube-proxy``` pod select karta hai. Fir request ko forward hoti hai selected pod ip pe.

Request pehle NodePort pe aati hai, fir kube-proxy ke iptables/ipvs rules ye decide karte hain ki us request ko ClusterIP pe redirect karna hai, aur us ClusterIP ke selector ke through kis pod pe bhejna hai.

**Step-4: Kube-proxy forward the traffic to pod**:

Ab ```kube-proxy``` traffic ko pod par forward karta hai.

Agar Pod same Node pe nahi hai, to:
- Linux kernel ke routing table se packet cross-node jaata hai ```node-B``` pe.
- Yaha CNI plugin (flannel, calico, weave etc.) is packet ko virtual ethernet interface ke through pod tak le jaata hai.
- Agar Pod same Node pe hota, to directly veth bridge se pod mil jaata.

**Step-5: Pod ke andar container pe request aati hai**:

Container par request jati hai aur humko same path se hote hue response milta hai aur web aap application browser pe dekh paate hain.


<br>

### Visulaization of flow:

```
User
 |
 v
Cloud Load Balancer (LB)
 |
 v
Worker Node (randomly chosen)
 |
 v
NodePort (e.g. 31234)
 |
 v
kube-proxy (iptables/ipvs)
 |
 v
ClusterIP Service (my-nginx)
 |
 v
Pod (IP: 10.244.x.x, Port: 8080)
 |
 v
nginx container
 |
 v
Response sent back via same path
```

<br>
<br>

### Example Setup: How request flow from load balancer to pod:

**Nginx Deployement 3 replicas ke saath**:

Example:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 8080
```

**Tumne ek load balancer service banai**:

Example:
```
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 31234  # auto-generated
```

**Ab Request Flow Samjho ‚Äì Ekdum Step-by-Step**:

**Step 1: User Request Initiates**:

User request karta hai:
```
http://<LoadBalancer IP>:80
```

Cloud Load Balancer is request ko forward karta hai kisi bhi Node ke IP + NodePort (31234) pe.

Maan lo:
```
Request gaya ‚Üí node-A:31234
```

**Step 2: Node ke Kernel pe iptables rules trigger hote hain**:

Node-A pe kube-proxy ne pehle se ye rule bana rakha hai:
```
agar traffic nodePort 31234 pe aaye,
to usko redirect karo ClusterIP 10.96.0.10:80 pe
```
Explanation:
- Yani yaha pe ClusterIP ki IP sirf ek virtual target hai,
- Request directly ClusterIP pe nahi aayi,
- NodePort pe aayi aur phir kube-proxy ne ClusterIP ko use kiya.

**Step 3: kube-proxy phir ClusterIP se Pod select karta hai (via selector)**:
- ClusterIP (10.96.0.10) service ke paas selector app=nginx hota hai.
- kube-proxy ke paas ek list hoti hai:
```
my-nginx service selector ‚Üí pods:
  - 10.244.1.15:8080
  - 10.244.1.16:8080
  - 10.244.2.11:8080
```

kube-proxy randomly ek pod choose karta hai:
```
‚Üí 10.244.1.15:8080
```

Aur kube-proxy us packet ko waha forward kar deta hai.

**Important Observations**:

| Question                        | Answer                                                                     |
| ------------------------------- | -------------------------------------------------------------------------- |
| Request pehle NodePort pe aayi? | ‚úÖ Yes                                                                     |
| kube-proxy ne kya dekha pehle?  | üîç NodePort ‚Üí uske rule me tha ClusterIP                                   |
| ClusterIP kya karta hai?        | üß† It is a **virtual identity** that has selectors to identify pods        |
| Pod kaise select hua?           | kube-proxy ne selector ke through pod list dekhi aur usme se 1 choose kiya |

<br>
<br>

## Example (LAB): Deploying a Simple Nginx Web Application Using LoadBalancer

  - **Create a Deployment for Your App using** ```nginx-deployment.yaml```:

    ```
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: nginx-deployment
      spec:
        replicas: 3                            # Creates 3 replicas
        selector:
          matchLabels:
            app: nginx-app
        template:
          metadata:
            labels:
              app: nginx-app                   # Label for identifying replicas
          spec:
            containers:
            - name: nginx
              image: nginx:latest              # Uses the latest NGINX image
              ports:
              - containerPort: 80              # NGINX listens on port 80 inside each pod
    ```

    Explanation of the Deployment YAML:

      - replicas: 3 means three copies of the NGINX server will run in the cluster.
      - selector & labels allow the service to know which pods to connect to (we‚Äôll use this in the next step).
      - containerPort: 80 is the port where NGINX listens for incoming traffic.

    To apply this file, run:

      ```kubectl apply -f nginx-deployment.yaml```

    This command creates the deployment with three NGINX pods.

  - **Create a LoadBalancer Service**

    Now, let‚Äôs create a LoadBalancer service that will give our NGINX app a public IP and distribute traffic across the replicas.

    Service File (nginx-loadbalancer-service.yaml):
    ```
      apiVersion: v1
      kind: Service
      metadata:
        name: nginx-loadbalancer
      spec:
        type: LoadBalancer                     # Exposes the service to the internet
        selector:
          app: nginx-app                       # Connects to pods with this label
        ports:
        - protocol: TCP
          port: 80                             # External port for the service
          targetPort: 80  
    ```

    Explanation of the Service YAML:

      - type: LoadBalancer tells Kubernetes to set up an external IP for this service.
      - selector: app: nginx-app connects the service to our NGINX pods using the label.
      - port: 80 is the port on the LoadBalancer‚Äôs IP that users will access.
      - targetPort: 80 directs the traffic to port 80 inside each NGINX pod.

    To create this service, run:

      ```kubectl apply -f nginx-loadbalancer-service.yaml```

  - **Check the External IP Address**

    Once the LoadBalancer service is created, Kubernetes will work with the cloud provider to set up a public IP address. You can check if the IP address is ready by running:

      ```kubectl get svc nginx-loadbalancer```

    You‚Äôll see output similar to this:

      ```
        NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE
        nginx-loadbalancer   LoadBalancer   10.96.95.215     52.23.145.198    80:31005/TCP   2m

      ```

    The EXTERNAL-IP (in this case, 52.23.145.198) is your app‚Äôs public IP.


  - **Access the App Using the External IP**

    You can access the NGINX web server using the external IP:

      ```http://52.23.145.198```

    Or, using curl in your terminal:

      ```curl http://52.23.145.198```
